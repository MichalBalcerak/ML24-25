{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+F4MDdUamHu4H66VN0oZu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalBalcerak/ML24-25/blob/main/Homework10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training LeNet5 model"
      ],
      "metadata": {
        "id": "ut2ch0ZL3Z23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8x-RfaO0zBs",
        "outputId": "888c5c95-259b-44fe-f6d9-be4526ffd7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 339kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.69MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.12MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot\n",
        "\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=2048,\n",
        "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=1,\n",
        "                                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet5(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels= 1, out_channels= 6, kernel_size = 5)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels= 6, out_channels= 16, kernel_size = 5)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels= 16, out_channels= 120, kernel_size = 4)\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "        self.avgpool = torch.nn.AvgPool2d(kernel_size= 2, stride = 2)\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(120, 80)\n",
        "        self.linear2 = torch.nn.Linear(80,10)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.relu(self.conv1(x))     # B, 6, 24, 24\n",
        "        x = self.avgpool(x)               # B, 6, 12, 12\n",
        "        x = self.relu(self.conv2(x))      # B, 16, 8, 8\n",
        "        x = self.avgpool(x)               # B, 16, 4, 4\n",
        "        x = self.relu(self.conv3(x))      # B, 120, 1, 1\n",
        "\n",
        "        x = x.squeeze(-1).squeeze(-1)     # B, 120\n",
        "\n",
        "        x = self.relu(self.linear1(x))    # B, 80\n",
        "        x = self.linear2(x)               # B, 10\n",
        "\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gE1MN1PL01lK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net = LeNet5().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001.\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(16):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)  #explicitly moving the data to the target device\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the LeNet5 object. Please note,\n",
        "                                            # the nonlinear activation after the last layer is NOT applied\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "                                ####You can experiment - comment this line and check, that the loss DOE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P3-jVmf04mN",
        "outputId": "42662203-7a5b-4810-a7c7-61fcdaf88b72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cpu\n",
            "epoch: 0 batch: 0 current batch loss: 2.3044567108154297\n",
            "epoch: 0 batch: 1 current batch loss: 2.2964251041412354\n",
            "epoch: 0 batch: 2 current batch loss: 2.2890684604644775\n",
            "epoch: 0 batch: 3 current batch loss: 2.2800509929656982\n",
            "epoch: 0 batch: 4 current batch loss: 2.2729737758636475\n",
            "epoch: 0 batch: 5 current batch loss: 2.259610652923584\n",
            "epoch: 0 batch: 6 current batch loss: 2.242342948913574\n",
            "epoch: 0 batch: 7 current batch loss: 2.224454641342163\n",
            "epoch: 0 batch: 8 current batch loss: 2.202317237854004\n",
            "epoch: 0 batch: 9 current batch loss: 2.1775426864624023\n",
            "epoch: 0 batch: 10 current batch loss: 2.1499242782592773\n",
            "epoch: 0 batch: 11 current batch loss: 2.1157116889953613\n",
            "epoch: 0 batch: 12 current batch loss: 2.0781326293945312\n",
            "epoch: 0 batch: 13 current batch loss: 2.0337820053100586\n",
            "epoch: 0 batch: 14 current batch loss: 1.9712457656860352\n",
            "epoch: 0 batch: 15 current batch loss: 1.9188660383224487\n",
            "epoch: 0 batch: 16 current batch loss: 1.8578848838806152\n",
            "epoch: 0 batch: 17 current batch loss: 1.7771347761154175\n",
            "epoch: 0 batch: 18 current batch loss: 1.687856674194336\n",
            "epoch: 0 batch: 19 current batch loss: 1.615186333656311\n",
            "epoch: 0 batch: 20 current batch loss: 1.522655725479126\n",
            "epoch: 0 batch: 21 current batch loss: 1.4270915985107422\n",
            "epoch: 0 batch: 22 current batch loss: 1.3552278280258179\n",
            "epoch: 0 batch: 23 current batch loss: 1.235312819480896\n",
            "epoch: 0 batch: 24 current batch loss: 1.1448262929916382\n",
            "epoch: 0 batch: 25 current batch loss: 1.082482933998108\n",
            "epoch: 0 batch: 26 current batch loss: 1.0106370449066162\n",
            "epoch: 0 batch: 27 current batch loss: 0.9813249707221985\n",
            "epoch: 0 batch: 28 current batch loss: 0.8584262728691101\n",
            "epoch: 0 batch: 29 current batch loss: 0.8414236307144165\n",
            "epoch: 1 batch: 0 current batch loss: 0.7970182299613953\n",
            "epoch: 1 batch: 1 current batch loss: 0.7638073563575745\n",
            "epoch: 1 batch: 2 current batch loss: 0.726000964641571\n",
            "epoch: 1 batch: 3 current batch loss: 0.7194719314575195\n",
            "epoch: 1 batch: 4 current batch loss: 0.7081109881401062\n",
            "epoch: 1 batch: 5 current batch loss: 0.6718314290046692\n",
            "epoch: 1 batch: 6 current batch loss: 0.6515352129936218\n",
            "epoch: 1 batch: 7 current batch loss: 0.6083554029464722\n",
            "epoch: 1 batch: 8 current batch loss: 0.6112170219421387\n",
            "epoch: 1 batch: 9 current batch loss: 0.6037949919700623\n",
            "epoch: 1 batch: 10 current batch loss: 0.5787143111228943\n",
            "epoch: 1 batch: 11 current batch loss: 0.595110297203064\n",
            "epoch: 1 batch: 12 current batch loss: 0.5123080015182495\n",
            "epoch: 1 batch: 13 current batch loss: 0.5365290641784668\n",
            "epoch: 1 batch: 14 current batch loss: 0.5517244338989258\n",
            "epoch: 1 batch: 15 current batch loss: 0.5284852385520935\n",
            "epoch: 1 batch: 16 current batch loss: 0.4972509443759918\n",
            "epoch: 1 batch: 17 current batch loss: 0.5261255502700806\n",
            "epoch: 1 batch: 18 current batch loss: 0.47251951694488525\n",
            "epoch: 1 batch: 19 current batch loss: 0.4956412613391876\n",
            "epoch: 1 batch: 20 current batch loss: 0.4583979547023773\n",
            "epoch: 1 batch: 21 current batch loss: 0.4886349141597748\n",
            "epoch: 1 batch: 22 current batch loss: 0.5145684480667114\n",
            "epoch: 1 batch: 23 current batch loss: 0.4119405448436737\n",
            "epoch: 1 batch: 24 current batch loss: 0.4470822215080261\n",
            "epoch: 1 batch: 25 current batch loss: 0.43487945199012756\n",
            "epoch: 1 batch: 26 current batch loss: 0.47909849882125854\n",
            "epoch: 1 batch: 27 current batch loss: 0.4197036921977997\n",
            "epoch: 1 batch: 28 current batch loss: 0.45773404836654663\n",
            "epoch: 1 batch: 29 current batch loss: 0.38987940549850464\n",
            "epoch: 2 batch: 0 current batch loss: 0.4141315519809723\n",
            "epoch: 2 batch: 1 current batch loss: 0.457563191652298\n",
            "epoch: 2 batch: 2 current batch loss: 0.42370888590812683\n",
            "epoch: 2 batch: 3 current batch loss: 0.4197387397289276\n",
            "epoch: 2 batch: 4 current batch loss: 0.4154248535633087\n",
            "epoch: 2 batch: 5 current batch loss: 0.42654895782470703\n",
            "epoch: 2 batch: 6 current batch loss: 0.407185822725296\n",
            "epoch: 2 batch: 7 current batch loss: 0.41916266083717346\n",
            "epoch: 2 batch: 8 current batch loss: 0.39604294300079346\n",
            "epoch: 2 batch: 9 current batch loss: 0.39409464597702026\n",
            "epoch: 2 batch: 10 current batch loss: 0.4110519587993622\n",
            "epoch: 2 batch: 11 current batch loss: 0.41222861409187317\n",
            "epoch: 2 batch: 12 current batch loss: 0.3909303545951843\n",
            "epoch: 2 batch: 13 current batch loss: 0.3922409415245056\n",
            "epoch: 2 batch: 14 current batch loss: 0.403486043214798\n",
            "epoch: 2 batch: 15 current batch loss: 0.35930830240249634\n",
            "epoch: 2 batch: 16 current batch loss: 0.33150428533554077\n",
            "epoch: 2 batch: 17 current batch loss: 0.38104957342147827\n",
            "epoch: 2 batch: 18 current batch loss: 0.3892269432544708\n",
            "epoch: 2 batch: 19 current batch loss: 0.32394275069236755\n",
            "epoch: 2 batch: 20 current batch loss: 0.36486896872520447\n",
            "epoch: 2 batch: 21 current batch loss: 0.34728991985321045\n",
            "epoch: 2 batch: 22 current batch loss: 0.3906083405017853\n",
            "epoch: 2 batch: 23 current batch loss: 0.34424012899398804\n",
            "epoch: 2 batch: 24 current batch loss: 0.31443947553634644\n",
            "epoch: 2 batch: 25 current batch loss: 0.3458271324634552\n",
            "epoch: 2 batch: 26 current batch loss: 0.328677237033844\n",
            "epoch: 2 batch: 27 current batch loss: 0.35184094309806824\n",
            "epoch: 2 batch: 28 current batch loss: 0.3510008454322815\n",
            "epoch: 2 batch: 29 current batch loss: 0.3969438672065735\n",
            "epoch: 3 batch: 0 current batch loss: 0.3517357110977173\n",
            "epoch: 3 batch: 1 current batch loss: 0.3470141291618347\n",
            "epoch: 3 batch: 2 current batch loss: 0.32659024000167847\n",
            "epoch: 3 batch: 3 current batch loss: 0.32973387837409973\n",
            "epoch: 3 batch: 4 current batch loss: 0.30021989345550537\n",
            "epoch: 3 batch: 5 current batch loss: 0.3160884380340576\n",
            "epoch: 3 batch: 6 current batch loss: 0.36745181679725647\n",
            "epoch: 3 batch: 7 current batch loss: 0.3307592272758484\n",
            "epoch: 3 batch: 8 current batch loss: 0.33745506405830383\n",
            "epoch: 3 batch: 9 current batch loss: 0.3396534025669098\n",
            "epoch: 3 batch: 10 current batch loss: 0.3109605014324188\n",
            "epoch: 3 batch: 11 current batch loss: 0.2895781397819519\n",
            "epoch: 3 batch: 12 current batch loss: 0.2925370931625366\n",
            "epoch: 3 batch: 13 current batch loss: 0.33882707357406616\n",
            "epoch: 3 batch: 14 current batch loss: 0.28344833850860596\n",
            "epoch: 3 batch: 15 current batch loss: 0.2891136109828949\n",
            "epoch: 3 batch: 16 current batch loss: 0.30417540669441223\n",
            "epoch: 3 batch: 17 current batch loss: 0.3226549029350281\n",
            "epoch: 3 batch: 18 current batch loss: 0.30454421043395996\n",
            "epoch: 3 batch: 19 current batch loss: 0.28614816069602966\n",
            "epoch: 3 batch: 20 current batch loss: 0.339782178401947\n",
            "epoch: 3 batch: 21 current batch loss: 0.298971563577652\n",
            "epoch: 3 batch: 22 current batch loss: 0.29958683252334595\n",
            "epoch: 3 batch: 23 current batch loss: 0.29094380140304565\n",
            "epoch: 3 batch: 24 current batch loss: 0.2791140079498291\n",
            "epoch: 3 batch: 25 current batch loss: 0.30719244480133057\n",
            "epoch: 3 batch: 26 current batch loss: 0.2691135108470917\n",
            "epoch: 3 batch: 27 current batch loss: 0.3087713420391083\n",
            "epoch: 3 batch: 28 current batch loss: 0.2530147135257721\n",
            "epoch: 3 batch: 29 current batch loss: 0.29551538825035095\n",
            "epoch: 4 batch: 0 current batch loss: 0.2788369357585907\n",
            "epoch: 4 batch: 1 current batch loss: 0.271089643239975\n",
            "epoch: 4 batch: 2 current batch loss: 0.26810503005981445\n",
            "epoch: 4 batch: 3 current batch loss: 0.25540679693222046\n",
            "epoch: 4 batch: 4 current batch loss: 0.29570499062538147\n",
            "epoch: 4 batch: 5 current batch loss: 0.26597076654434204\n",
            "epoch: 4 batch: 6 current batch loss: 0.2962400019168854\n",
            "epoch: 4 batch: 7 current batch loss: 0.28266146779060364\n",
            "epoch: 4 batch: 8 current batch loss: 0.26448890566825867\n",
            "epoch: 4 batch: 9 current batch loss: 0.26776063442230225\n",
            "epoch: 4 batch: 10 current batch loss: 0.2904590368270874\n",
            "epoch: 4 batch: 11 current batch loss: 0.2868185341358185\n",
            "epoch: 4 batch: 12 current batch loss: 0.24915221333503723\n",
            "epoch: 4 batch: 13 current batch loss: 0.2644955515861511\n",
            "epoch: 4 batch: 14 current batch loss: 0.28081485629081726\n",
            "epoch: 4 batch: 15 current batch loss: 0.2607015073299408\n",
            "epoch: 4 batch: 16 current batch loss: 0.24296188354492188\n",
            "epoch: 4 batch: 17 current batch loss: 0.278011292219162\n",
            "epoch: 4 batch: 18 current batch loss: 0.2482810765504837\n",
            "epoch: 4 batch: 19 current batch loss: 0.2216055691242218\n",
            "epoch: 4 batch: 20 current batch loss: 0.291157990694046\n",
            "epoch: 4 batch: 21 current batch loss: 0.2558611333370209\n",
            "epoch: 4 batch: 22 current batch loss: 0.27158910036087036\n",
            "epoch: 4 batch: 23 current batch loss: 0.22832246124744415\n",
            "epoch: 4 batch: 24 current batch loss: 0.23753732442855835\n",
            "epoch: 4 batch: 25 current batch loss: 0.2342596799135208\n",
            "epoch: 4 batch: 26 current batch loss: 0.24708199501037598\n",
            "epoch: 4 batch: 27 current batch loss: 0.25182026624679565\n",
            "epoch: 4 batch: 28 current batch loss: 0.25764888525009155\n",
            "epoch: 4 batch: 29 current batch loss: 0.23793503642082214\n",
            "epoch: 5 batch: 0 current batch loss: 0.2541619837284088\n",
            "epoch: 5 batch: 1 current batch loss: 0.22353771328926086\n",
            "epoch: 5 batch: 2 current batch loss: 0.23853212594985962\n",
            "epoch: 5 batch: 3 current batch loss: 0.19994865357875824\n",
            "epoch: 5 batch: 4 current batch loss: 0.23090624809265137\n",
            "epoch: 5 batch: 5 current batch loss: 0.2324814349412918\n",
            "epoch: 5 batch: 6 current batch loss: 0.2323748916387558\n",
            "epoch: 5 batch: 7 current batch loss: 0.24663391709327698\n",
            "epoch: 5 batch: 8 current batch loss: 0.21425822377204895\n",
            "epoch: 5 batch: 9 current batch loss: 0.2183254361152649\n",
            "epoch: 5 batch: 10 current batch loss: 0.2264067679643631\n",
            "epoch: 5 batch: 11 current batch loss: 0.2398039549589157\n",
            "epoch: 5 batch: 12 current batch loss: 0.24521537125110626\n",
            "epoch: 5 batch: 13 current batch loss: 0.2188766449689865\n",
            "epoch: 5 batch: 14 current batch loss: 0.25676172971725464\n",
            "epoch: 5 batch: 15 current batch loss: 0.23677441477775574\n",
            "epoch: 5 batch: 16 current batch loss: 0.199820876121521\n",
            "epoch: 5 batch: 17 current batch loss: 0.23036442697048187\n",
            "epoch: 5 batch: 18 current batch loss: 0.23413728177547455\n",
            "epoch: 5 batch: 19 current batch loss: 0.18230341374874115\n",
            "epoch: 5 batch: 20 current batch loss: 0.24859142303466797\n",
            "epoch: 5 batch: 21 current batch loss: 0.22503940761089325\n",
            "epoch: 5 batch: 22 current batch loss: 0.22374750673770905\n",
            "epoch: 5 batch: 23 current batch loss: 0.20347166061401367\n",
            "epoch: 5 batch: 24 current batch loss: 0.21854522824287415\n",
            "epoch: 5 batch: 25 current batch loss: 0.21532775461673737\n",
            "epoch: 5 batch: 26 current batch loss: 0.2015480399131775\n",
            "epoch: 5 batch: 27 current batch loss: 0.23708108067512512\n",
            "epoch: 5 batch: 28 current batch loss: 0.20687609910964966\n",
            "epoch: 5 batch: 29 current batch loss: 0.19348262250423431\n",
            "epoch: 6 batch: 0 current batch loss: 0.20338211953639984\n",
            "epoch: 6 batch: 1 current batch loss: 0.18376991152763367\n",
            "epoch: 6 batch: 2 current batch loss: 0.21546795964241028\n",
            "epoch: 6 batch: 3 current batch loss: 0.20646600425243378\n",
            "epoch: 6 batch: 4 current batch loss: 0.19985263049602509\n",
            "epoch: 6 batch: 5 current batch loss: 0.21355943381786346\n",
            "epoch: 6 batch: 6 current batch loss: 0.19945259392261505\n",
            "epoch: 6 batch: 7 current batch loss: 0.23524822294712067\n",
            "epoch: 6 batch: 8 current batch loss: 0.20479634404182434\n",
            "epoch: 6 batch: 9 current batch loss: 0.20319439470767975\n",
            "epoch: 6 batch: 10 current batch loss: 0.2012292742729187\n",
            "epoch: 6 batch: 11 current batch loss: 0.19317413866519928\n",
            "epoch: 6 batch: 12 current batch loss: 0.1991327852010727\n",
            "epoch: 6 batch: 13 current batch loss: 0.18505358695983887\n",
            "epoch: 6 batch: 14 current batch loss: 0.20318901538848877\n",
            "epoch: 6 batch: 15 current batch loss: 0.1895514279603958\n",
            "epoch: 6 batch: 16 current batch loss: 0.1996224969625473\n",
            "epoch: 6 batch: 17 current batch loss: 0.19991432130336761\n",
            "epoch: 6 batch: 18 current batch loss: 0.20789936184883118\n",
            "epoch: 6 batch: 19 current batch loss: 0.22235530614852905\n",
            "epoch: 6 batch: 20 current batch loss: 0.17362749576568604\n",
            "epoch: 6 batch: 21 current batch loss: 0.1939849704504013\n",
            "epoch: 6 batch: 22 current batch loss: 0.1917305737733841\n",
            "epoch: 6 batch: 23 current batch loss: 0.1808038204908371\n",
            "epoch: 6 batch: 24 current batch loss: 0.19113433361053467\n",
            "epoch: 6 batch: 25 current batch loss: 0.20303033292293549\n",
            "epoch: 6 batch: 26 current batch loss: 0.18003514409065247\n",
            "epoch: 6 batch: 27 current batch loss: 0.19302800297737122\n",
            "epoch: 6 batch: 28 current batch loss: 0.22437326610088348\n",
            "epoch: 6 batch: 29 current batch loss: 0.19840677082538605\n",
            "epoch: 7 batch: 0 current batch loss: 0.1720608025789261\n",
            "epoch: 7 batch: 1 current batch loss: 0.17792724072933197\n",
            "epoch: 7 batch: 2 current batch loss: 0.16596072912216187\n",
            "epoch: 7 batch: 3 current batch loss: 0.17474602162837982\n",
            "epoch: 7 batch: 4 current batch loss: 0.1827412247657776\n",
            "epoch: 7 batch: 5 current batch loss: 0.18792176246643066\n",
            "epoch: 7 batch: 6 current batch loss: 0.17700552940368652\n",
            "epoch: 7 batch: 7 current batch loss: 0.19079630076885223\n",
            "epoch: 7 batch: 8 current batch loss: 0.17875277996063232\n",
            "epoch: 7 batch: 9 current batch loss: 0.19175206124782562\n",
            "epoch: 7 batch: 10 current batch loss: 0.14973346889019012\n",
            "epoch: 7 batch: 11 current batch loss: 0.19449859857559204\n",
            "epoch: 7 batch: 12 current batch loss: 0.16657783091068268\n",
            "epoch: 7 batch: 13 current batch loss: 0.20426301658153534\n",
            "epoch: 7 batch: 14 current batch loss: 0.16659733653068542\n",
            "epoch: 7 batch: 15 current batch loss: 0.17044474184513092\n",
            "epoch: 7 batch: 16 current batch loss: 0.16716285049915314\n",
            "epoch: 7 batch: 17 current batch loss: 0.17044401168823242\n",
            "epoch: 7 batch: 18 current batch loss: 0.18515022099018097\n",
            "epoch: 7 batch: 19 current batch loss: 0.16259489953517914\n",
            "epoch: 7 batch: 20 current batch loss: 0.17471538484096527\n",
            "epoch: 7 batch: 21 current batch loss: 0.16779671609401703\n",
            "epoch: 7 batch: 22 current batch loss: 0.1762474775314331\n",
            "epoch: 7 batch: 23 current batch loss: 0.17078307271003723\n",
            "epoch: 7 batch: 24 current batch loss: 0.16192540526390076\n",
            "epoch: 7 batch: 25 current batch loss: 0.17554621398448944\n",
            "epoch: 7 batch: 26 current batch loss: 0.1743583232164383\n",
            "epoch: 7 batch: 27 current batch loss: 0.14974555373191833\n",
            "epoch: 7 batch: 28 current batch loss: 0.17543594539165497\n",
            "epoch: 7 batch: 29 current batch loss: 0.1529611051082611\n",
            "epoch: 8 batch: 0 current batch loss: 0.16687312722206116\n",
            "epoch: 8 batch: 1 current batch loss: 0.1874275803565979\n",
            "epoch: 8 batch: 2 current batch loss: 0.16845539212226868\n",
            "epoch: 8 batch: 3 current batch loss: 0.17151106894016266\n",
            "epoch: 8 batch: 4 current batch loss: 0.15655003488063812\n",
            "epoch: 8 batch: 5 current batch loss: 0.17767825722694397\n",
            "epoch: 8 batch: 6 current batch loss: 0.16489221155643463\n",
            "epoch: 8 batch: 7 current batch loss: 0.15712033212184906\n",
            "epoch: 8 batch: 8 current batch loss: 0.14907504618167877\n",
            "epoch: 8 batch: 9 current batch loss: 0.17029370367527008\n",
            "epoch: 8 batch: 10 current batch loss: 0.15543575584888458\n",
            "epoch: 8 batch: 11 current batch loss: 0.16752876341342926\n",
            "epoch: 8 batch: 12 current batch loss: 0.15071891248226166\n",
            "epoch: 8 batch: 13 current batch loss: 0.1473071277141571\n",
            "epoch: 8 batch: 14 current batch loss: 0.16447357833385468\n",
            "epoch: 8 batch: 15 current batch loss: 0.16208623349666595\n",
            "epoch: 8 batch: 16 current batch loss: 0.16172625124454498\n",
            "epoch: 8 batch: 17 current batch loss: 0.18150997161865234\n",
            "epoch: 8 batch: 18 current batch loss: 0.16031688451766968\n",
            "epoch: 8 batch: 19 current batch loss: 0.15774892270565033\n",
            "epoch: 8 batch: 20 current batch loss: 0.16133977472782135\n",
            "epoch: 8 batch: 21 current batch loss: 0.1497555524110794\n",
            "epoch: 8 batch: 22 current batch loss: 0.13745753467082977\n",
            "epoch: 8 batch: 23 current batch loss: 0.14695121347904205\n",
            "epoch: 8 batch: 24 current batch loss: 0.16113515198230743\n",
            "epoch: 8 batch: 25 current batch loss: 0.1455543488264084\n",
            "epoch: 8 batch: 26 current batch loss: 0.14760930836200714\n",
            "epoch: 8 batch: 27 current batch loss: 0.1606946438550949\n",
            "epoch: 8 batch: 28 current batch loss: 0.14442265033721924\n",
            "epoch: 8 batch: 29 current batch loss: 0.1601817011833191\n",
            "epoch: 9 batch: 0 current batch loss: 0.16980263590812683\n",
            "epoch: 9 batch: 1 current batch loss: 0.16390003263950348\n",
            "epoch: 9 batch: 2 current batch loss: 0.1677379459142685\n",
            "epoch: 9 batch: 3 current batch loss: 0.17660892009735107\n",
            "epoch: 9 batch: 4 current batch loss: 0.14540089666843414\n",
            "epoch: 9 batch: 5 current batch loss: 0.15540222823619843\n",
            "epoch: 9 batch: 6 current batch loss: 0.17219677567481995\n",
            "epoch: 9 batch: 7 current batch loss: 0.15927790105342865\n",
            "epoch: 9 batch: 8 current batch loss: 0.1493958681821823\n",
            "epoch: 9 batch: 9 current batch loss: 0.143896222114563\n",
            "epoch: 9 batch: 10 current batch loss: 0.15609991550445557\n",
            "epoch: 9 batch: 11 current batch loss: 0.15464890003204346\n",
            "epoch: 9 batch: 12 current batch loss: 0.13043060898780823\n",
            "epoch: 9 batch: 13 current batch loss: 0.13732552528381348\n",
            "epoch: 9 batch: 14 current batch loss: 0.12100343406200409\n",
            "epoch: 9 batch: 15 current batch loss: 0.16579970717430115\n",
            "epoch: 9 batch: 16 current batch loss: 0.13250552117824554\n",
            "epoch: 9 batch: 17 current batch loss: 0.16433723270893097\n",
            "epoch: 9 batch: 18 current batch loss: 0.13791441917419434\n",
            "epoch: 9 batch: 19 current batch loss: 0.1607827991247177\n",
            "epoch: 9 batch: 20 current batch loss: 0.1312868595123291\n",
            "epoch: 9 batch: 21 current batch loss: 0.13993176817893982\n",
            "epoch: 9 batch: 22 current batch loss: 0.1486474722623825\n",
            "epoch: 9 batch: 23 current batch loss: 0.15936318039894104\n",
            "epoch: 9 batch: 24 current batch loss: 0.15571452677249908\n",
            "epoch: 9 batch: 25 current batch loss: 0.11836915463209152\n",
            "epoch: 9 batch: 26 current batch loss: 0.1478225737810135\n",
            "epoch: 9 batch: 27 current batch loss: 0.1486181765794754\n",
            "epoch: 9 batch: 28 current batch loss: 0.14013558626174927\n",
            "epoch: 9 batch: 29 current batch loss: 0.13217701017856598\n",
            "epoch: 10 batch: 0 current batch loss: 0.1279541701078415\n",
            "epoch: 10 batch: 1 current batch loss: 0.1302899718284607\n",
            "epoch: 10 batch: 2 current batch loss: 0.13459540903568268\n",
            "epoch: 10 batch: 3 current batch loss: 0.14115476608276367\n",
            "epoch: 10 batch: 4 current batch loss: 0.13887691497802734\n",
            "epoch: 10 batch: 5 current batch loss: 0.11850504577159882\n",
            "epoch: 10 batch: 6 current batch loss: 0.11642996221780777\n",
            "epoch: 10 batch: 7 current batch loss: 0.1364055871963501\n",
            "epoch: 10 batch: 8 current batch loss: 0.14727286994457245\n",
            "epoch: 10 batch: 9 current batch loss: 0.1208663135766983\n",
            "epoch: 10 batch: 10 current batch loss: 0.1345185786485672\n",
            "epoch: 10 batch: 11 current batch loss: 0.1348648965358734\n",
            "epoch: 10 batch: 12 current batch loss: 0.1537814736366272\n",
            "epoch: 10 batch: 13 current batch loss: 0.12011005729436874\n",
            "epoch: 10 batch: 14 current batch loss: 0.15582317113876343\n",
            "epoch: 10 batch: 15 current batch loss: 0.12916159629821777\n",
            "epoch: 10 batch: 16 current batch loss: 0.14213494956493378\n",
            "epoch: 10 batch: 17 current batch loss: 0.11719249933958054\n",
            "epoch: 10 batch: 18 current batch loss: 0.12605617940425873\n",
            "epoch: 10 batch: 19 current batch loss: 0.1209820955991745\n",
            "epoch: 10 batch: 20 current batch loss: 0.14119566977024078\n",
            "epoch: 10 batch: 21 current batch loss: 0.14096719026565552\n",
            "epoch: 10 batch: 22 current batch loss: 0.13127626478672028\n",
            "epoch: 10 batch: 23 current batch loss: 0.13717326521873474\n",
            "epoch: 10 batch: 24 current batch loss: 0.14040736854076385\n",
            "epoch: 10 batch: 25 current batch loss: 0.12608227133750916\n",
            "epoch: 10 batch: 26 current batch loss: 0.15292376279830933\n",
            "epoch: 10 batch: 27 current batch loss: 0.1532021313905716\n",
            "epoch: 10 batch: 28 current batch loss: 0.1132703348994255\n",
            "epoch: 10 batch: 29 current batch loss: 0.12938950955867767\n",
            "epoch: 11 batch: 0 current batch loss: 0.11852305382490158\n",
            "epoch: 11 batch: 1 current batch loss: 0.1079716831445694\n",
            "epoch: 11 batch: 2 current batch loss: 0.12768998742103577\n",
            "epoch: 11 batch: 3 current batch loss: 0.1373625248670578\n",
            "epoch: 11 batch: 4 current batch loss: 0.14073790609836578\n",
            "epoch: 11 batch: 5 current batch loss: 0.14148494601249695\n",
            "epoch: 11 batch: 6 current batch loss: 0.12061991542577744\n",
            "epoch: 11 batch: 7 current batch loss: 0.1377088725566864\n",
            "epoch: 11 batch: 8 current batch loss: 0.1225094199180603\n",
            "epoch: 11 batch: 9 current batch loss: 0.1335245668888092\n",
            "epoch: 11 batch: 10 current batch loss: 0.1350080370903015\n",
            "epoch: 11 batch: 11 current batch loss: 0.12206579744815826\n",
            "epoch: 11 batch: 12 current batch loss: 0.13366864621639252\n",
            "epoch: 11 batch: 13 current batch loss: 0.15074191987514496\n",
            "epoch: 11 batch: 14 current batch loss: 0.13083963096141815\n",
            "epoch: 11 batch: 15 current batch loss: 0.11224038153886795\n",
            "epoch: 11 batch: 16 current batch loss: 0.12724973261356354\n",
            "epoch: 11 batch: 17 current batch loss: 0.12762072682380676\n",
            "epoch: 11 batch: 18 current batch loss: 0.12538941204547882\n",
            "epoch: 11 batch: 19 current batch loss: 0.12429080903530121\n",
            "epoch: 11 batch: 20 current batch loss: 0.10669222474098206\n",
            "epoch: 11 batch: 21 current batch loss: 0.1392482966184616\n",
            "epoch: 11 batch: 22 current batch loss: 0.13407553732395172\n",
            "epoch: 11 batch: 23 current batch loss: 0.12379924207925797\n",
            "epoch: 11 batch: 24 current batch loss: 0.1272217035293579\n",
            "epoch: 11 batch: 25 current batch loss: 0.11538684368133545\n",
            "epoch: 11 batch: 26 current batch loss: 0.12116608023643494\n",
            "epoch: 11 batch: 27 current batch loss: 0.11254958808422089\n",
            "epoch: 11 batch: 28 current batch loss: 0.13865132629871368\n",
            "epoch: 11 batch: 29 current batch loss: 0.11029627174139023\n",
            "epoch: 12 batch: 0 current batch loss: 0.12074793130159378\n",
            "epoch: 12 batch: 1 current batch loss: 0.1362142413854599\n",
            "epoch: 12 batch: 2 current batch loss: 0.1382482498884201\n",
            "epoch: 12 batch: 3 current batch loss: 0.10222610831260681\n",
            "epoch: 12 batch: 4 current batch loss: 0.11827582120895386\n",
            "epoch: 12 batch: 5 current batch loss: 0.11179959774017334\n",
            "epoch: 12 batch: 6 current batch loss: 0.12382015585899353\n",
            "epoch: 12 batch: 7 current batch loss: 0.11798238009214401\n",
            "epoch: 12 batch: 8 current batch loss: 0.09777382761240005\n",
            "epoch: 12 batch: 9 current batch loss: 0.13604934513568878\n",
            "epoch: 12 batch: 10 current batch loss: 0.11286367475986481\n",
            "epoch: 12 batch: 11 current batch loss: 0.11489446461200714\n",
            "epoch: 12 batch: 12 current batch loss: 0.11873327195644379\n",
            "epoch: 12 batch: 13 current batch loss: 0.10945764929056168\n",
            "epoch: 12 batch: 14 current batch loss: 0.10876662284135818\n",
            "epoch: 12 batch: 15 current batch loss: 0.13042378425598145\n",
            "epoch: 12 batch: 16 current batch loss: 0.11324314773082733\n",
            "epoch: 12 batch: 17 current batch loss: 0.11253411322832108\n",
            "epoch: 12 batch: 18 current batch loss: 0.11325062811374664\n",
            "epoch: 12 batch: 19 current batch loss: 0.11008469015359879\n",
            "epoch: 12 batch: 20 current batch loss: 0.12516583502292633\n",
            "epoch: 12 batch: 21 current batch loss: 0.1287681758403778\n",
            "epoch: 12 batch: 22 current batch loss: 0.13481861352920532\n",
            "epoch: 12 batch: 23 current batch loss: 0.09481361508369446\n",
            "epoch: 12 batch: 24 current batch loss: 0.12271717935800552\n",
            "epoch: 12 batch: 25 current batch loss: 0.10701435059309006\n",
            "epoch: 12 batch: 26 current batch loss: 0.09862425178289413\n",
            "epoch: 12 batch: 27 current batch loss: 0.12780632078647614\n",
            "epoch: 12 batch: 28 current batch loss: 0.11278032511472702\n",
            "epoch: 12 batch: 29 current batch loss: 0.10574808716773987\n",
            "epoch: 13 batch: 0 current batch loss: 0.10202896595001221\n",
            "epoch: 13 batch: 1 current batch loss: 0.10929073393344879\n",
            "epoch: 13 batch: 2 current batch loss: 0.11697127670049667\n",
            "epoch: 13 batch: 3 current batch loss: 0.12791207432746887\n",
            "epoch: 13 batch: 4 current batch loss: 0.11948269605636597\n",
            "epoch: 13 batch: 5 current batch loss: 0.11445236206054688\n",
            "epoch: 13 batch: 6 current batch loss: 0.10449057072401047\n",
            "epoch: 13 batch: 7 current batch loss: 0.09027877449989319\n",
            "epoch: 13 batch: 8 current batch loss: 0.10721859335899353\n",
            "epoch: 13 batch: 9 current batch loss: 0.10946371406316757\n",
            "epoch: 13 batch: 10 current batch loss: 0.12546798586845398\n",
            "epoch: 13 batch: 11 current batch loss: 0.11266402155160904\n",
            "epoch: 13 batch: 12 current batch loss: 0.09255850315093994\n",
            "epoch: 13 batch: 13 current batch loss: 0.12262550741434097\n",
            "epoch: 13 batch: 14 current batch loss: 0.09753409028053284\n",
            "epoch: 13 batch: 15 current batch loss: 0.11705006659030914\n",
            "epoch: 13 batch: 16 current batch loss: 0.09813690185546875\n",
            "epoch: 13 batch: 17 current batch loss: 0.10902220010757446\n",
            "epoch: 13 batch: 18 current batch loss: 0.11732318252325058\n",
            "epoch: 13 batch: 19 current batch loss: 0.10131409764289856\n",
            "epoch: 13 batch: 20 current batch loss: 0.10921382904052734\n",
            "epoch: 13 batch: 21 current batch loss: 0.12995196878910065\n",
            "epoch: 13 batch: 22 current batch loss: 0.10488533973693848\n",
            "epoch: 13 batch: 23 current batch loss: 0.11705756932497025\n",
            "epoch: 13 batch: 24 current batch loss: 0.11296885460615158\n",
            "epoch: 13 batch: 25 current batch loss: 0.11806972324848175\n",
            "epoch: 13 batch: 26 current batch loss: 0.10945771634578705\n",
            "epoch: 13 batch: 27 current batch loss: 0.13255257904529572\n",
            "epoch: 13 batch: 28 current batch loss: 0.10172077268362045\n",
            "epoch: 13 batch: 29 current batch loss: 0.09146993607282639\n",
            "epoch: 14 batch: 0 current batch loss: 0.09609359502792358\n",
            "epoch: 14 batch: 1 current batch loss: 0.10289035737514496\n",
            "epoch: 14 batch: 2 current batch loss: 0.09616410732269287\n",
            "epoch: 14 batch: 3 current batch loss: 0.10991396754980087\n",
            "epoch: 14 batch: 4 current batch loss: 0.10546495765447617\n",
            "epoch: 14 batch: 5 current batch loss: 0.10850413888692856\n",
            "epoch: 14 batch: 6 current batch loss: 0.09980573505163193\n",
            "epoch: 14 batch: 7 current batch loss: 0.12816205620765686\n",
            "epoch: 14 batch: 8 current batch loss: 0.10859236121177673\n",
            "epoch: 14 batch: 9 current batch loss: 0.09306282550096512\n",
            "epoch: 14 batch: 10 current batch loss: 0.12074120342731476\n",
            "epoch: 14 batch: 11 current batch loss: 0.09422681480646133\n",
            "epoch: 14 batch: 12 current batch loss: 0.10281436890363693\n",
            "epoch: 14 batch: 13 current batch loss: 0.1106918603181839\n",
            "epoch: 14 batch: 14 current batch loss: 0.1026579812169075\n",
            "epoch: 14 batch: 15 current batch loss: 0.09823764115571976\n",
            "epoch: 14 batch: 16 current batch loss: 0.11159884184598923\n",
            "epoch: 14 batch: 17 current batch loss: 0.12187252938747406\n",
            "epoch: 14 batch: 18 current batch loss: 0.10972341150045395\n",
            "epoch: 14 batch: 19 current batch loss: 0.10279292613267899\n",
            "epoch: 14 batch: 20 current batch loss: 0.09851422905921936\n",
            "epoch: 14 batch: 21 current batch loss: 0.097031369805336\n",
            "epoch: 14 batch: 22 current batch loss: 0.10501828789710999\n",
            "epoch: 14 batch: 23 current batch loss: 0.10577920824289322\n",
            "epoch: 14 batch: 24 current batch loss: 0.10927096009254456\n",
            "epoch: 14 batch: 25 current batch loss: 0.12238408625125885\n",
            "epoch: 14 batch: 26 current batch loss: 0.0963553935289383\n",
            "epoch: 14 batch: 27 current batch loss: 0.11417336761951447\n",
            "epoch: 14 batch: 28 current batch loss: 0.0878380537033081\n",
            "epoch: 14 batch: 29 current batch loss: 0.08722367882728577\n",
            "epoch: 15 batch: 0 current batch loss: 0.11235430836677551\n",
            "epoch: 15 batch: 1 current batch loss: 0.10000055283308029\n",
            "epoch: 15 batch: 2 current batch loss: 0.10370326042175293\n",
            "epoch: 15 batch: 3 current batch loss: 0.12657597661018372\n",
            "epoch: 15 batch: 4 current batch loss: 0.10705679655075073\n",
            "epoch: 15 batch: 5 current batch loss: 0.12190500646829605\n",
            "epoch: 15 batch: 6 current batch loss: 0.0794719010591507\n",
            "epoch: 15 batch: 7 current batch loss: 0.10862082242965698\n",
            "epoch: 15 batch: 8 current batch loss: 0.1309286504983902\n",
            "epoch: 15 batch: 9 current batch loss: 0.09998531639575958\n",
            "epoch: 15 batch: 10 current batch loss: 0.10236269980669022\n",
            "epoch: 15 batch: 11 current batch loss: 0.0815521627664566\n",
            "epoch: 15 batch: 12 current batch loss: 0.10785173624753952\n",
            "epoch: 15 batch: 13 current batch loss: 0.09680712223052979\n",
            "epoch: 15 batch: 14 current batch loss: 0.08456343412399292\n",
            "epoch: 15 batch: 15 current batch loss: 0.09974149614572525\n",
            "epoch: 15 batch: 16 current batch loss: 0.11109615117311478\n",
            "epoch: 15 batch: 17 current batch loss: 0.1308673769235611\n",
            "epoch: 15 batch: 18 current batch loss: 0.08510861545801163\n",
            "epoch: 15 batch: 19 current batch loss: 0.09859609603881836\n",
            "epoch: 15 batch: 20 current batch loss: 0.10199861973524094\n",
            "epoch: 15 batch: 21 current batch loss: 0.09746846556663513\n",
            "epoch: 15 batch: 22 current batch loss: 0.12244679778814316\n",
            "epoch: 15 batch: 23 current batch loss: 0.0861106589436531\n",
            "epoch: 15 batch: 24 current batch loss: 0.09065868705511093\n",
            "epoch: 15 batch: 25 current batch loss: 0.0972856879234314\n",
            "epoch: 15 batch: 26 current batch loss: 0.09466192871332169\n",
            "epoch: 15 batch: 27 current batch loss: 0.11468706279993057\n",
            "epoch: 15 batch: 28 current batch loss: 0.11243637651205063\n",
            "epoch: 15 batch: 29 current batch loss: 0.11928144097328186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint.to(device))                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MtHmio_09EA",
        "outputId": "ec1de606-f602-40d7-fe80-8f0ee0c32829"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9786\n"
          ]
        }
      ]
    }
  ]
}