{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiBZr9ru0pY1sIjT3AsgUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalBalcerak/ML24-25/blob/main/Homework07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have\n",
        "\n",
        "\n",
        "*   A data set $\\{y_i\\}_{i=1}^n$ of binary labels $y_i\\in\\{0,1\\}$\n",
        "*   Constants: $m$ - the number of ones $\\{i: y_i=1\\}$, $\\quad$ $k$ - the numbers of zeros $\\{i: y_i=0\\}$\n",
        "* A set of predictions $f_i=f_{m-1}(x_i)$ obtained from a previous stage where the $f_i$  values are generated randomly from a normal distribution\n",
        "\n",
        "## A: Fitting a Constant Predictor from Scratch\n",
        "\n",
        "The task is to find the optimal constant value $\\lambda$ that minimizes the binary cross-entropy loss over the dataset:\n",
        "\n",
        "$$\n",
        "\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, \\lambda).\n",
        "$$\n",
        "\n",
        "where loss function $L$ is defined as:\n",
        "$$L(y, z) = -y \\log(\\sigma(z)) - (1 - y) \\log(1 - \\sigma(z))$$\n",
        "\n",
        "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
        "\n",
        "In order to find the minimum of the function $\\sum_{i=1}^{n} L(y_i, \\lambda)$, I compute first and second derrivative of $L=L(y_i,\\lambda)$ with respect to $\\lambda$:\n",
        "\n",
        "\n",
        "$$\\frac{\\partial L(y_i,\\lambda)}{\\partial\\lambda}=\\frac{e^{\\lambda}(1-y_i)-y_i}{e^{\\lambda}+1}$$\n",
        "\n",
        "\n",
        "$$\\frac{\\partial^2 L(y_i,\\lambda)}{\\partial\\lambda^2}=\\frac{e^\\lambda}{(e^\\lambda+1)^2}$$\n",
        "\n",
        "Since $\\frac{\\partial^2 L(y_i,\\lambda)}{\\partial\\lambda^2}>0$ for all $i$, we also have\n",
        "$$\\frac{\\partial^2}{\\partial\\lambda^2}\\sum_{i=1}^{n} L(y_i, \\lambda)=\\sum_{i=1}^{n}\\frac{\\partial^2}{\\partial\\lambda^2}L(y_i, \\lambda)>0$$\n",
        "\n",
        "thus the examined function is strictly convex - its global minimum is in critical point, i.e. $\\lambda^*$ such that\n",
        "$$\\frac{\\partial }{\\partial\\lambda}\\sum_{i=1}^{n} L(y_i, \\lambda)=\\sum_{i=1}^{n}\\frac{\\partial }{\\partial\\lambda} L(y_i, \\lambda)=\\sum_{i=1}^{n}\\frac{e^{\\lambda}(1-y_i)-y_i}{e^{\\lambda}+1}=0$$\n",
        "\n",
        "Therefore:\n",
        "$$\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, \\lambda)=\\log\\left(\\frac{\\sum_{i=1}^n y_i}{\\sum_{i=1}^n (1-y_i)}\\right)$$\n",
        "\n",
        "which, with above notation, is equal to\n",
        "\n",
        "$$\\lambda^*=\\log\\left(\\frac{m}{k}\\right)$$\n",
        "\n",
        "Thus, in terms of the dataset's label distribution, this constant represents class 1 ($y_i=1$) to class 2 ($y_i=0$) ratio."
      ],
      "metadata": {
        "id": "zXI6iEdUm62a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B: Fitting the m-th Predictor in the Forward Stagewise Procedure\n",
        "\n",
        "Assume we have an existing predictor $f_i=f_{m-1}(x_i)$. We would like to find an optimal additive correction $\\lambda$ such that the updated prediction for each data point becomes\n",
        "$$\n",
        " f_i + \\lambda,\n",
        "$$\n",
        "\n",
        "and the corresponding binary cross-entropy loss is given by\n",
        "\n",
        "$$\n",
        "\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, f_i + \\lambda).\n",
        "$$\n",
        "\n",
        "As in the previous situation, I compute first and second derrivative of $L=L(y_i,f_i+\\lambda)$ with respect to $\\lambda$:\n",
        "\n",
        "$$\\frac{\\partial L(y_i,\\lambda)}{\\partial\\lambda}=\\frac{e^{f_i+\\lambda}(1-y_i)-y_i}{e^{f_i+\\lambda}+1}$$\n",
        "\n",
        "\n",
        "$$\\frac{\\partial^2 L(y_i,\\lambda)}{\\partial\\lambda^2}=\\frac{e^{f_i+\\lambda}}{(e^{f_i+\\lambda}+1)^2}$$\n",
        "\n",
        "As in the above situation, examined function $\\sum_{i=1}^{n} L(y_i, f_i + \\lambda)$ is strictly convex because we have:\n",
        "\n",
        "$$\\frac{\\partial^2}{\\partial\\lambda^2}\\sum_{i=1}^{n} L(y_i, f_i+\\lambda)=\\sum_{i=1}^{n}\\frac{\\partial^2}{\\partial\\lambda^2}L(y_i, f_i+\\lambda)>0$$\n",
        "\n",
        "Therefore desired minimizer $\\lambda^*$ is given by the following equation:\n",
        "$$\\frac{\\partial }{\\partial\\lambda}\\sum_{i=1}^{n} L(y_i, f_i+\\lambda)=\\sum_{i=1}^{n}\\frac{\\partial }{\\partial\\lambda} L(y_i, f_i+\\lambda)=\\sum_{i=1}^{n}\\frac{e^{f_i+\\lambda}(1-y_i)-y_i}{e^{f_i+\\lambda}+1}=0$$\n",
        "\n",
        "However, due to the fact that $f_i$ is different for various indices $i$ and non-linearity of the sigmoid function in the binary cross-entropy loss, we are not able to find the solution of the above eqaution."
      ],
      "metadata": {
        "id": "Ckt2UwQyB45i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W9O4VgGiDTgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSt6S-RhE9Ny"
      },
      "outputs": [],
      "source": []
    }
  ]
}